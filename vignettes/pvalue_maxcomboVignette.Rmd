---
title: Computing p-values for Fleming-Harring weighted logrank tests and the MaxCombo test
output: rmarkdown::html_vignette
bibliography: simtrial.bib
vignette: >
  %\VignetteIndexEntry{Computing p-values for Fleming-Harring weighted logrank tests and the MaxCombo test}
  %\VignetteEngine{knitr::rmarkdown} 
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

This vignette demonstrates use of a simple routine to do simulations and testing using Fleming-Harrington weighted logrank tests and the MaxCombo test.
In addition, we demonstrate how to perform these tests with a dataset not generated by simulation routines within the package.
Note that all p-values computed here are one-sided with small values indicating that the experimental treatment is favored. 

## Defining the test

The MaxCombo test has been posed as the maximum of multiple Fleming-Harrington weighted logrank tests (@FH1982, @FH2011).
Combination tests looking at a maximum of selected tests in this class have also been proposed; see @Lee2007, @NPHWGDesign, and @NPHWGSimulation.
The Fleming-Harrington class is indexed by the parameters $\rho\ge 0$ and $\gamma\ge 0$.
We will denote these as FH($\rho,\gamma$). 
This class includes the logrank test as FH(0,0).
Other tests of interest here include:

- FH(0,1): a test that down-weights early events
- FH(1,0): a test that down-weights late events
- FH(1,1): a test that down-weights events increasingly as their quantiles differ from the median

## Executing for a single dataset

### Generating test statistics with `sim_fixed_n()`

We begin with a single trial simulation generated by the routine `sim_fixed_n()` using default arguments for that routine.
`sim_fixed_n()` produces one record per test and data cutoff method per simulation.
Here we choose 3 tests (logrank=FH(0,0), FH(0,1) and FH(1,1)).
When more than one test is chosen the correlation between tests is computed as shown by @Karrison2016, in this case in the columns `V1, V2, V3`. The columns `rho, gamma` indicate $\rho$ and $\gamma$ used to compute the test. `z` is the FH($\rho,\gamma$) normal test statistic with variance 1 with a negative value favoring experimental treatment. The variable `cut` indicates how the data were cut for analysis, in this case at the maximum of the targeted minimum follow-up after last enrollment and the date at which the targeted event count was reached. `Sim` is a sequential index of the simulations performed.  

```{r, message=FALSE, warning=FALSE}
library(simtrial)
library(knitr)
library(dplyr)
```

```{r}
x <- sim_fixed_n(n_sim = 1, timing_type = 5, rho_gamma = tibble::tibble(rho = c(0, 0, 1), gamma = c(0, 1, 1)))
x %>% kable(digits = 2)
```

Once you have this format, the MaxCombo p-value per @Karrison2016, @NPHWGDesign can be computed as follows (note that you will need to have the package **mvtnorm** installed):

```{r,warning=FALSE,message=FALSE}
pvalue_maxcombo(x)
```

### Generating data with `simtrial::sim_pw_surv()`

We begin with another simulation generated by `simtrial::sim_pw_surv()`.
Again, we use defaults for that routine.

```{r,message=FALSE,warning=FALSE,cache=FALSE}
s <- sim_pw_surv(n = 100)
head(s) %>% kable(digits = 2)
```

Once generated, we need to cut the data for analysis. Here we cut after 75 events.

```{r,warning=FALSE,message=FALSE}
x <- s %>% cut_data_by_event(75)
head(x) %>% kable(digits = 2)
```

Now we can analyze this data. We begin with `s` to show how this can be done in a single line.
In this case, we use the 4 test combination suggested in @NPHWGSimulation, @NPHWGDesign.

```{r,warning=FALSE,message=FALSE}
z <- s %>%
  cut_data_by_event(75) %>%
  counting_process(arm = "experimental") %>%
  tenFHcorr(rho_gamma = tibble(rho = c(0, 0, 1, 1), gamma = c(0, 1, 0, 1)))
z %>% kable(digits = 2)
```

Now we compute our p-value as before:

```{r,warning=FALSE,message=FALSE}
pvalue_maxcombo(z)
```

Suppose we want the p-value just based on the logrank and FH(0,1) and FH(1,0) as suggested by @Lee2007.
We remove the rows and columns associated with FH(0,0) and FH(1,1) and then apply `pvalue_maxcombo()`.

```{r,warning=FALSE,message=FALSE}
pvalue_maxcombo(z %>% select(-c(V1, V4)) %>% filter((rho == 0 & gamma == 1) | (rho == 1 & gamma == 0)))
```

### Using survival data in another format

For a trial not generated by `sim_fixed_n()`, the process is slightly more involved.
We consider survival data not in the **simtrial** format and show the transformation needed.
In this case we use the small `aml` dataset from the **survival** package.

```{r,warning=FALSE,message=FALSE}
library(survival)
head(aml) %>% kable()
```

We rename variables and create a stratum variable as follows:

```{r,warning=FALSE,message=FALSE}
x <- aml %>% transmute(tte = time, event = status, stratum = "All", treatment = as.character(x))
head(x) %>% kable()
```
Now we analyze the data with a MaxCombo with the logrank and FH(0,1) and compute a p-value.

```{r,warning=FALSE,message=FALSE}
x %>%
  counting_process(arm = "Maintained") %>%
  tenFHcorr(rho_gamma = tibble(rho = 0, gamma = c(0, 1))) %>%
  pvalue_maxcombo()
```

## Simulation

We now consider the example simulation from the `pvalue_maxcombo()` help file to demonstrate how to simulate power for the MaxCombo test. However, we increase the number of simulations to 100 in this case; a larger number should be used (e.g., 1000) for a better estimate of design properties. Here we will test at the $\alpha=0.001$ level.

```{r,cache=FALSE,warning=FALSE,message=FALSE}
# Only use cut events + min follow-up
xx <- sim_fixed_n(n_sim = 100, timing_type = 5, rho_gamma = tibble(rho = c(0, 0, 1), gamma = c(0, 1, 1)))
# MaxCombo power estimate for cutoff at max of targeted events, minimum follow-up
p <- unlist(xx %>% group_by(Sim) %>% group_map(pvalue_maxcombo))
mean(p < .001)
```

We note the use of `group_map` in the above produces a list of p-values for each simulation.
It would be nice to have something that worked more like `dplyr::summarize()` to avoid `unlist()` and to allow evaluating, say, multiple data cutoff methods.
The latter can be done without having to re-run all simulations as follows, demonstrated with a smaller number of simulations.

```{r,cache=FALSE,warning=FALSE,message=FALSE}
# Only use cuts for events and events + min follow-up
xx <- sim_fixed_n(n_sim = 100, timing_type = c(2, 5), rho_gamma = tibble(rho = 0, gamma = c(0, 1)))
head(xx) %>% kable(digits = 2)
```

Now we compute a p-value separately for each cut type, first for targeted event count.

```{r,warning=FALSE,message=FALSE}
# Subset to targeted events cutoff tests
p <- unlist(xx %>% filter(cut == "Targeted events") %>% group_by(Sim) %>% group_map(pvalue_maxcombo))
mean(p < .025)
```

Now we use the later of targeted events and minimum follow-up cutoffs.

```{r,warning=FALSE,message=FALSE}
# Subset to targeted events cutoff tests
p <- unlist(xx %>% filter(cut != "Targeted events") %>% group_by(Sim) %>% group_map(pvalue_maxcombo))
mean(p < .025)
```

## References
